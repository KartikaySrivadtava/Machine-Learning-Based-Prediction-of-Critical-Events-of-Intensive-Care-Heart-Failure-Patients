{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold, cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, BayesianRidge, Ridge, SGDClassifier, LinearRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from random import randint\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, RobustScaler, KBinsDiscretizer, MinMaxScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eccb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataframe\n",
    "\n",
    "df = pd.read_csv('MIMIC_WITHOUT_OUTLIERS_ITERATIVE_IMPUTATION.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb45a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the dataframe into train and test\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "#Split the data into train and test based on GROUPID \n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7)\n",
    "split = splitter.split(df, groups=df['GROUP_ID'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train = df.iloc[train_inds]\n",
    "test = df.iloc[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd24ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X and Y for machine learning evaluation\n",
    "\n",
    "X_train = train[['AGE', 'GENDER_M', 'MARITAL_STATUS_DIVORCED', 'MARITAL_STATUS_LIFE PARTNER',\n",
    "       'MARITAL_STATUS_MARRIED', 'MARITAL_STATUS_SEPARATED',\n",
    "       'MARITAL_STATUS_SINGLE', 'MARITAL_STATUS_UNKNOWN (DEFAULT)',\n",
    "       'MARITAL_STATUS_WIDOWED', 'REL_DAY', 'BMI', 'HEART_RATE',\n",
    "       'FLAG_HEART_RATE_ALARM_LOW', 'FLAG_HEART_RATE_ALARM_HIGH',\n",
    "       'OXYGEN_SATURATION', 'FLAG_OXYGEN_SATURATION_ALARM_HIGH','FLAG_OXYGEN_SATURATION_ALARM_LOW', \n",
    "        'ARTERIAL_BLOOD_PRESSURE_SYSTOLIC', 'ARTERIAL_BLOOD_PRESSURE_DIASTOLIC']]\n",
    "\n",
    "Y_train = train['DOD_LABEL']\n",
    "\n",
    "testX = test[['AGE', 'GENDER_M', 'MARITAL_STATUS_DIVORCED', 'MARITAL_STATUS_LIFE PARTNER',\n",
    "       'MARITAL_STATUS_MARRIED', 'MARITAL_STATUS_SEPARATED',\n",
    "       'MARITAL_STATUS_SINGLE', 'MARITAL_STATUS_UNKNOWN (DEFAULT)',\n",
    "       'MARITAL_STATUS_WIDOWED', 'REL_DAY', 'BMI', 'HEART_RATE',\n",
    "       'FLAG_HEART_RATE_ALARM_LOW', 'FLAG_HEART_RATE_ALARM_HIGH',\n",
    "       'OXYGEN_SATURATION', 'FLAG_OXYGEN_SATURATION_ALARM_HIGH','FLAG_OXYGEN_SATURATION_ALARM_LOW', \n",
    "        'ARTERIAL_BLOOD_PRESSURE_SYSTOLIC', 'ARTERIAL_BLOOD_PRESSURE_DIASTOLIC']]\n",
    "\n",
    "testY = test['DOD_LABEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75051deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_ids = df['GROUP_ID']\n",
    "\n",
    "# define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit on the training dataset\n",
    "scaler.fit(X_train)\n",
    "# scale the training dataset\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "cv = list(GroupKFold(n_splits=4).split(X_train,Y_train,pat_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6133f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best hyperparameters\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# define models and parameters\n",
    "model = LogisticRegression()\n",
    "parameters = {'C': np.logspace(-2, 0, 20),\n",
    "              'penalty': ['none', 'l2', 'l1', 'elasticnet'],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "              'multi_class': ['multinomial']}\n",
    "\n",
    "# define grid search\n",
    "grid_search = GridSearchCV(model, parameters, cv=cv, verbose=0, n_jobs=-1)\n",
    "log_reg_cv = grid_search.fit(X_train, Y_train)\n",
    "# summarize results\n",
    "print(\"Tuned hpyerparameters (best parameters):\", log_reg_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16cc81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Classifier\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "model = RidgeClassifier()\n",
    "alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "# define grid search\n",
    "grid = dict(alpha=alpha)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,error_score=0)\n",
    "rig_clf_cv = grid_search.fit(X_train, Y_train)\n",
    "# summarize results\n",
    "print(\"Tuned hpyerparameters (best parameters):\", rig_clf_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b28aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "kernel = ['poly', 'rbf', 'sigmoid', 'linear']\n",
    "C = [50, 10, 1.0, 0.1, 0.01]\n",
    "gamma = ['scale']\n",
    "\n",
    "# define grid search\n",
    "grid = dict(kernel=kernel,C=C,gamma=gamma)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,error_score=0)\n",
    "svm_cv = grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Tuned hpyerparameters (best parameters):\", svm_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "parameters = {'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "              'max_depth': [2*n for n in range(1,10)],\n",
    "              'max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# define grid search\n",
    "grid_search = GridSearchCV(model, parameters, n_jobs=-1, cv=cv,error_score=0)\n",
    "rf_cv = grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Tuned hpyerparameters (best parameters):\", rf_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca232b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors': list(range(1, 20)),\n",
    "              'weights': ['uniform', 'distance'],\n",
    "              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "              'p': [1,2]}\n",
    "\n",
    "#define gid search\n",
    "grid_search = GridSearchCV(model, parameters, n_jobs=-1, cv=cv,error_score=0)\n",
    "knn_cv = grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Tuned hpyerparameters (best parameters):\", knn_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "param_grid = { 'max_depth': [4,5,6] , 'min_child_weight':[4,5,6] ,'learning_rate': [0.05,0.1,0.5] ,'n_estimators': [20,50,100] }\n",
    "model=XGBClassifier()\n",
    "grid = GridSearchCV(model, param_grid, cv=cv, n_jobs=-1)\n",
    "#fit model to data\n",
    "xgb_cv = grid.fit(X_train, Y_train)\n",
    "print(\"Tuned hpyerparameters (best parameters):\", xgb_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]\n",
    "             ,\"splitter\":[\"best\"]}\n",
    " \n",
    "# Instantiating Decision Tree classifier\n",
    "tree = DecisionTreeClassifier()\n",
    " \n",
    "grid = GridSearchCV(model, param_grid, cv=cv, n_jobs=-1)\n",
    "#fit model to data\n",
    "xgb_cv = grid.fit(X_train, Y_train)\n",
    "print(\"Tuned hpyerparameters (best parameters):\", tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the best hyperparameters have been found, evaluation of Machine Learning models will be done to find best \n",
    "#AUROC score\n",
    "\n",
    "model_names = ['Logistic Regression', 'Ridge Classifier', 'KNN', 'Random Forest', 'SVM', 'XGBoost', 'Decision Tree']\n",
    "models = [LogisticRegression(C=0.03359818286283781, multi_class='multinomial',penalty='l2', solver='newton-cg'),\n",
    "         RidgeClassifier(alpha=0.5), \n",
    "         KNeighborsClassifier(algorithm='auto', n_neighbors= 14, p=1, weights='uniform'),\n",
    "         RandomForestClassifier(criterion='gini', max_depth=12, max_features='sqrt', min_samples_leaf=4, \n",
    "                                       min_samples_split=10),\n",
    "         SVC(C=0.1, gamma='scale', kernel='linear'),\n",
    "         XGBClassifier(learning_rate=0.1, max_depth=5, min_child_weight=4, n_estimators=20),\n",
    "         DecisionTreeClassifier(criterion='entropy', max_depth=18, max_features='auto', min_samples_leaf=1, min_samples_split=2,\n",
    "                              splitter='best')\n",
    "         ]\n",
    "\n",
    "print('Results of ad hoc modeling:')\n",
    "\n",
    "##########################################Find AUROC charecteristics################\n",
    "\n",
    "#print('\\t\\t\\tX-val AUROC || train AUROC   ||   X-val AUPRC || train AUPRC')\n",
    "#print('-----------------------------------------------------------------------------------------')\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model_name = model_names[i]\n",
    "    scores = cross_validate(model, X_train, Y_train, scoring=['roc_auc', 'average_precision'], \n",
    "                            cv=list(GroupKFold(n_splits=4).split(X_train,Y_train,pat_ids)), return_train_score=True, return_estimator=False, n_jobs=-1)\n",
    "    print(\"Model Name: \", model_name)\n",
    "    print('{0:<20s}: {1:.2f} +/- {2:.2f} || {3:.2f} +/- {4:.2f} || {5:.2f} +/- {6:.2f} || {7:.2f} +/- {8:.2f}'.format(model_name, \n",
    "                                                                       scores['test_roc_auc'].mean(), scores['test_roc_auc'].std(),\n",
    "                                                                       scores['train_roc_auc'].mean(), scores['train_roc_auc'].std(), \n",
    "                                                                       scores['test_average_precision'].mean(), scores['test_average_precision'].std(),\n",
    "                                                                       scores['train_average_precision'].mean(), scores['train_average_precision'].std())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After finding the AUROC scores for these models, AUROC plots will be drawn for these models\n",
    "\n",
    "def draw_cv_roc_curve(classifier, cv, X_train, Y_train, title='ROC Curve'):\n",
    "    # Creating ROC Curve with Cross Validation\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    i = 0\n",
    "    for train, test in cv.split(X_train, Y_train,pat_ids):\n",
    "        probas_ = classifier.fit(X_train.iloc[train], Y_train.iloc[train]).predict_proba(X_train.iloc[test])\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(y.iloc[test], probas_[:, 1])\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "        i += 1\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Luck', alpha=.8)\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='b',label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=2, alpha=.8)\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,label=r'$\\pm$ 1 std. dev.')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "clf=LogisticRegression(C=0.03359818286283781, multi_class='multinomial', penalty='l2', solver='newton-cg')\n",
    "# Set up Stratified K Fold\n",
    "cv = GroupKFold(n_splits=4)\n",
    "draw_cv_roc_curve(clf, cv, X_train, Y_train, title='Cross Validated LogisticRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e80cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "\n",
    "clf=KNeighborsClassifier(algorithm='auto', n_neighbors= 14, p=1, weights='uniform')\n",
    "# Set up Stratified K Fold\n",
    "cv = GroupKFold(n_splits=4)\n",
    "draw_cv_roc_curve(clf, cv, X_train, Y_train, title='Cross Validated KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf69529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest\n",
    "\n",
    "clf=RandomForestClassifier(criterion='gini', max_depth=12, max_features='sqrt', min_samples_leaf=4, min_samples_split=10)\n",
    "# Set up Stratified K Fold\n",
    "cv = GroupKFold(n_splits=4)\n",
    "draw_cv_roc_curve(clf, cv, X_train, Y_train, title='Cross Validated RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "\n",
    "clf=XGBClassifier(learning_rate=0.1, max_depth=5, min_child_weight=4, n_estimators=20)\n",
    "# Set up Stratified K Fold\n",
    "cv = GroupKFold(n_splits=4)\n",
    "draw_cv_roc_curve(clf, cv, X_train, Y_train, title='Cross Validated XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c83720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DecisionTree\n",
    "\n",
    "clf=DecisionTreeClassifier(criterion='entropy', max_depth=18, max_features='auto', min_samples_leaf=1, \n",
    "                           min_samples_split=2, splitter='best')\n",
    "# Set up Stratified K Fold\n",
    "cv = GroupKFold(n_splits=4)\n",
    "draw_cv_roc_curve(clf, cv, X_train, Y_train, title='Cross Validated DecisionTreeClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RidgeClassifier\n",
    "\n",
    "clf = LogisticRegression(penalty='l2')\n",
    "cv = GroupKFold(n_splits=4)\n",
    "draw_cv_roc_curve(clf, cv, X_train, Y_train, title='Cross Validated RidgeClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e84244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "\n",
    "clf=SVC(C=0.1, gamma='scale', kernel='linear', probability=True)\n",
    "# Set up Stratified K Fold\n",
    "cv = GroupKFold(n_splits=4)\n",
    "draw_cv_roc_curve(clf, cv, X_train, Y_train, title='Cross Validated SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5db172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the models\n",
    "\n",
    "lr=LogisticRegression(C=0.03359818286283781, multi_class='multinomial',penalty='l2', solver='newton-cg').fit(X_train, Y_train)\n",
    "rc=LogisticRegression(penalty='l2').fit(X_train, Y_train)\n",
    "knn=KNeighborsClassifier(algorithm='auto', n_neighbors= 14, p=1, weights='uniform').fit(X_train, Y_train)\n",
    "rf=RandomForestClassifier(criterion='gini', max_depth=12, max_features='sqrt', min_samples_leaf=4, \n",
    "                                       min_samples_split=10).fit(X_train, Y_train)\n",
    "svm=SVC(C=0.1, gamma='scale', kernel='linear', probability=True).fit(X_train, Y_train)\n",
    "xgb=XGBClassifier(learning_rate=0.1, max_depth=5, min_child_weight=4, n_estimators=20).fit(X_train, Y_train)\n",
    "dt=DecisionTreeClassifier(criterion='entropy', max_depth=18, max_features='auto', min_samples_leaf=1, min_samples_split=2,\n",
    "                              splitter='best').fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d5c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the models using J statistics\n",
    "\n",
    "model_names = ['Logistic Regression', 'Ridge Classifier', 'KNN', 'Random Forest', 'SVM', 'XGBoost', 'Decision Tree']\n",
    "\n",
    "models = [lr, rc, knn, rf, svm, xgb, dt]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print('-------------------------------')\n",
    "    model_name = model_names[i]\n",
    "    print(\"Model Name is:\", model_name)\n",
    "    fpr, tpr, thresholds = roc_curve(testY, model.predict_proba(testX)[:, 1])\n",
    "    # get the best threshold\n",
    "    J = tpr - fpr\n",
    "    ix = np.argmax(J)\n",
    "    best_thresh = thresholds[ix]\n",
    "    print('Best Threshold=%f' % (best_thresh))\n",
    "    y_pred = (model.predict_proba(testX)[:, 1] > 0.008)\n",
    "    ConfusionMatrixDisplay.from_predictions(testY, y_pred, normalize='true', cmap='Blues', display_labels = model.classes_)\n",
    "    ConfusionMatrixDisplay.from_predictions(testY, y_pred, cmap='Blues', display_labels = model.classes_)\n",
    "    print(\"Accuracy is:\",accuracy_score(testY, y_pred))\n",
    "    print(\"Precision is:\",precision_score(testY, y_pred))\n",
    "    print(\"Recall is:\",recall_score(testY, y_pred))\n",
    "    print(\"F1 is:\",f1_score(testY, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
