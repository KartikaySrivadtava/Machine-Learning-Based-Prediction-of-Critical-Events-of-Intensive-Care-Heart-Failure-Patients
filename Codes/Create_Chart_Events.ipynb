{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06002ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the filtered admissions file which contains heart failure patients\n",
    "\n",
    "df_admissions = pd.read_csv('ADMISSIONS_FILTERED.csv')\n",
    "list_patients = df_admissions['SUBJECT_ID'].unique()\n",
    "len(list_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047021cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the CHARTEVENTS file for the required vital signs (please note that all the files have been created using the default \n",
    "#directory and the user needs to change accordingly).\n",
    "\n",
    "list_vital = [224167, 227242, 227243, 227537, 227538, 224751, \n",
    "              224639, 224643, 220045, 220046, 220047, 220050, \n",
    "              220051, 220052, 220056, 220058, 220224, 220227, \n",
    "              226512, 226707, 220277, 223769, 223770, 442, \n",
    "              443, 8440, 51, 8368, 52, 8555, 6701 , 6702, 53, \n",
    "              490, 646, 733, 3580, 762, 763, 3693, 5820, 8554, \n",
    "              211, 3494, 1394]\n",
    "\n",
    "columns = ['SUBJECT_ID', 'HADM_ID', 'ITEMID', 'CHARTTIME', 'VALUENUM', 'VALUEUOM']\n",
    "df = pd.read_csv(r\"CHARTEVENTS.csv\", chunksize=10000000, usecols=columns,\n",
    "                 low_memory=False)\n",
    "\n",
    "i = 1\n",
    "df_vitals=pd.DataFrame()\n",
    "\n",
    "for data in df:\n",
    "    print(\"Chunk no\", i)\n",
    "    data = data[data['SUBJECT_ID'].isin(list_patients) & data['ITEMID'].isin(list_vital)]\n",
    "    data.drop_duplicates(keep=False, inplace=True)\n",
    "    df_vitals=df_vitals.append(data,ignore_index=True)\n",
    "    i=i+1\n",
    "\n",
    "df2 = pd.read_csv('D_ITEMS.csv')\n",
    "df_vitals = pd.merge(df_vitals,df2[['ITEMID','LABEL']],on='ITEMID', how='left')\n",
    "df_vitals.to_csv('vitals_filtered_2.csv', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The above mentioned dataframe has been saved and then read again to avoid going through the whole process again and again\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('vitals_filtered_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd3546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleted column that is not required\n",
    "\n",
    "del df['VALUEUOM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb80afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start creating the required dataframe\n",
    "\n",
    "df_events_sub = df[df['CHARTTIME'] == df.groupby(['SUBJECT_ID', 'HADM_ID'])['CHARTTIME'].transform('min')] \n",
    "df_events_sub = df_events_sub.rename(columns= {'CHARTTIME':'CHARTTIME_START'})\n",
    "df_events_sub = df_events_sub[['SUBJECT_ID','HADM_ID','CHARTTIME_START']]\n",
    "df = df.merge(df_events_sub, how = 'left', right_on = ['SUBJECT_ID','HADM_ID'], left_on = ['SUBJECT_ID','HADM_ID']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18406d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert date_columns\n",
    "\n",
    "df['CHARTTIME_START'] = pd.to_datetime(df['CHARTTIME_START']) \n",
    "df['CHARTTIME'] = pd.to_datetime(df['CHARTTIME'])\n",
    "df['REL_DAY'] = (df['CHARTTIME']-df['CHARTTIME_START']).dt.days\n",
    "del df['CHARTTIME']\n",
    "del df['CHARTTIME_START']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de3a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe for manipulation\n",
    "\n",
    "df2 = df.groupby(['SUBJECT_ID', 'HADM_ID', 'REL_DAY','ITEMID', 'LABEL'], as_index=False).mean('VALUENUM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save this dataframe to avoid repeating the whole process\n",
    "\n",
    "df2.to_csv('vitals_demo_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30175c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restructure the dataframe to start creating the feature matrix and save the final dataframe\n",
    "\n",
    "df_restructured = df2.pivot_table(index=[\"SUBJECT_ID\", \"HADM_ID\", \"REL_DAY\"], columns=[\"LABEL\"],values=\"VALUENUM\").reset_index()\n",
    "df_restructured.to_csv('vitals_restructured_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6990956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start reading the filtered file again to create death time in ICU and save new dataframe with last chart time\n",
    "\n",
    "df_restructured = pd.read_csv('vitals_restructured_2.csv')\n",
    "df_max_day=(df_restructured.loc[df_restructured.groupby(['GROUP_ID'])['REL_DAY'].idxmax()])\n",
    "df_max_day_charttime = pd.merge(df_max_day, df_chart_max[['GROUP_ID','CHARTTIME']],on='GROUP_ID', how='left')\n",
    "df_max_day_charttime['CHARTTIME'] = pd.to_datetime(df_max_day_charttime['CHARTTIME']).dt.date\n",
    "df_max_day_charttime['CHARTTIME'] = pd.to_datetime(df_max_day_charttime['CHARTTIME']).dt.date\n",
    "df_max_day_charttime = df_max_day_charttime.rename(columns={'CHARTTIME': 'DEATHTIME'})\n",
    "df_max_day_charttime.to_csv('chart_time_last.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab61c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the filtered admissions file and check it with death time\n",
    "\n",
    "df_admissions = pd.read_csv('ADMISSIONS_FILTERED.csv')\n",
    "\n",
    "df_max_day_charttime = df_max_day_charttime[['SUBJECT_ID', 'HADM_ID', 'GROUP_ID', 'REL_DAY', 'DEATHTIME',\n",
    "       'ART Blood Pressure Alarm - High', 'ART Blood Pressure Alarm - Low',\n",
    "       'Admission Weight (Kg)', 'Admit Wt', 'Arterial BP #2 [Diastolic]',\n",
    "       'Arterial BP #2 [Systolic]', 'Arterial BP Mean', 'Arterial BP Mean #2',\n",
    "       'Arterial BP [Diastolic]', 'Arterial BP [Systolic]',\n",
    "       'Arterial Blood Pressure Alarm - High',\n",
    "       'Arterial Blood Pressure Alarm - Low',\n",
    "       'Arterial Blood Pressure diastolic', 'Arterial Blood Pressure mean',\n",
    "       'Arterial Blood Pressure systolic', 'Arterial O2 Saturation',\n",
    "       'Arterial O2 pressure', 'Daily Weight', 'Heart Rate',\n",
    "       'Heart Rate Alarm - Low', 'Heart rate Alarm - High', 'Height',\n",
    "       'Manual BP Mean(calc)', 'Manual BP [Diastolic]', 'Manual BP [Systolic]',\n",
    "       'Manual Blood Pressure Diastolic Left',\n",
    "       'Manual Blood Pressure Diastolic Right',\n",
    "       'Manual Blood Pressure Systolic Left',\n",
    "       'Manual Blood Pressure Systolic Right',\n",
    "       'O2 Saturation Pulseoxymetry Alarm - High',\n",
    "       'O2 Saturation Pulseoxymetry Alarm - Low',\n",
    "       'O2 saturation pulseoxymetry', 'PAO2', 'SpO2', 'SpO2 Alarm [High]',\n",
    "       'SpO2 Alarm [Low]', 'Temporary Pacemaker Rate', 'Weight Change']]\n",
    "\n",
    "final_df = pd.merge(df_max_day_charttime, df_admissions,  how='left', left_on=['GROUP_ID','DEATHTIME'], right_on = ['GROUP_ID','DEATHTIME'])\n",
    "\n",
    "df_admissions.to_csv('ADMISSIONS_FILTERED.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf05306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vitals = pd.read_csv('vitals_restructured_2.csv')\n",
    "df_max_day_charttime=pd.read_csv('chart_time_last.csv')\n",
    "df_adm = pd.read_csv('ADMISSIONS_FILTERED.csv')\n",
    "\n",
    "df_max_day_charttime.columns = df_max_day_charttime.columns.str.replace('DEATHTIME', 'LAST_CHART_TIME')\n",
    "df_max_day_charttime = df_max_day_charttime[['SUBJECT_ID', 'HADM_ID', 'GROUP_ID', 'REL_DAY','LAST_CHART_TIME']]\n",
    "res = df_max_day_charttime.merge(df_adm, how='inner', left_on=['GROUP_ID', 'LAST_CHART_TIME'], right_on=['GROUP_ID', 'DEATHTIME'])\n",
    "\n",
    "#Remove duplicate columns after merge operation\n",
    "res=res.drop(['SUBJECT_ID_y', 'HADM_ID_y', 'MARITAL_STATUS', 'GENDER', 'AGE'], axis=1)\n",
    "res = res.rename(columns={'SUBJECT_ID_x': 'SUBJECT_ID', 'HADM_ID_x': 'HADM_ID'})\n",
    "\n",
    "#Check if the last chart time matches with the death time recorded in the hospital to ensure only the patients who died \n",
    "#in the hopspital ICU have been included in this study\n",
    "\n",
    "res['LAST_CHART_TIME'].equals(res['DEATHTIME']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c30d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df_vitals.merge(res, how='inner', left_on=['GROUP_ID', 'REL_DAY'], right_on=['GROUP_ID', 'REL_DAY'])\n",
    "df2 = pd.merge(df_vitals, res, on=['GROUP_ID','REL_DAY'])\n",
    "\n",
    "#Create death chart csv file\n",
    "res.to_csv('chart_death.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e49f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now start creating final dataframe that contains the actual death time flagged along with the vital signs\n",
    "\n",
    "df_vitals = pd.read_csv('vitals_restructured_2.csv')\n",
    "df_chart_death = pd.read_csv('chart_death.csv')\n",
    "df = df_vitals.merge(df_chart_death, on=['GROUP_ID','REL_DAY'], how='outer')\n",
    "\n",
    "#Rename the columns which changed due to duplication\n",
    "df = df.rename(columns={'SUBJECT_ID_x': 'SUBJECT_ID', 'HADM_ID_x': 'HADM_ID'})\n",
    "\n",
    "#Remove redundant columns created due to the merging process\n",
    "df.drop(['SUBJECT_ID_y', 'HADM_ID_y'], axis=1, inplace=True)\n",
    "\n",
    "#The following steps have taken to fill null values with 0\n",
    "df['LAST_CHART_TIME'] = df['LAST_CHART_TIME'].fillna(0)\n",
    "df['DEATHTIME'] = df['DEATHTIME'].fillna(0)\n",
    "df['DEATH_IN_HOSPITAL'] = df['DEATH_IN_HOSPITAL'].fillna(0)\n",
    "\n",
    "df.to_csv('chart_events_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
